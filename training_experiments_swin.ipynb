{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code write by kushwanth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments for better training of swin ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T19:28:44.857382Z",
     "iopub.status.busy": "2025-04-25T19:28:44.857065Z",
     "iopub.status.idle": "2025-04-25T19:28:45.590984Z",
     "shell.execute_reply": "2025-04-25T19:28:45.589973Z",
     "shell.execute_reply.started": "2025-04-25T19:28:44.857359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Dataset (no cropping) ----------\n",
    "class DIV2KDataset(Dataset):\n",
    "    SUPPORTED_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "    def __init__(self, lr_dir, hr_dir):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        lr_files = {f for f in os.listdir(lr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        hr_files = {f for f in os.listdir(hr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        self.files = sorted(lr_files & hr_files)\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No matching images in {lr_dir} and {hr_dir}\")\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        lr = Image.open(os.path.join(self.lr_dir, fname)).convert(\"RGB\")  # assumed 256x256\n",
    "        hr = Image.open(os.path.join(self.hr_dir, fname)).convert(\"RGB\")  # assumed 1024x1024\n",
    "        return self.to_tensor(lr), self.to_tensor(hr)\n",
    "\n",
    "\n",
    "# ---------- SwinIR Model ----------\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans=3, embed_dim=96, patch_size=1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    B,C,H,W = x.shape\n",
    "    x = x.view(B, C, H//window_size, window_size, W//window_size, window_size)\n",
    "    windows = x.permute(0,2,4,3,5,1).contiguous().view(-1, window_size*window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H*W/window_size/window_size))\n",
    "    x = windows.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0,5,1,3,2,4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        self.heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.heads, C//self.heads).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1,2).reshape(B_, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=8, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        shortcut = x\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm1(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "        x = x + shortcut\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        return x\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    def __init__(self, dim, depth, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        layers = [SwinTransformerBlock(dim, num_heads, window_size) for _ in range(depth)]\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(self.blocks(x)) + x\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, scale, n_feats):\n",
    "        m = []\n",
    "        if (scale & (scale-1)) == 0:\n",
    "            for _ in range(int(math.log2(scale))):\n",
    "                m += [nn.Conv2d(n_feats, n_feats*4, 3,1,1), nn.PixelShuffle(2)]\n",
    "        elif scale == 3:\n",
    "            m += [nn.Conv2d(n_feats, n_feats*9, 3,1,1), nn.PixelShuffle(3)]\n",
    "        else:\n",
    "            raise ValueError(f\"Scale {scale} not supported\")\n",
    "        m += [nn.Conv2d(n_feats, 3, 3,1,1)]\n",
    "        super().__init__(*m)\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    def __init__(self, embed_dim=96, depths=[6,6,6,6], num_heads=[6,6,6,6], window_size=8, scale=4):\n",
    "        super().__init__()\n",
    "        self.shallow = PatchEmbed(3, embed_dim, 1)\n",
    "        self.body = nn.Sequential(*[\n",
    "            RSTB(embed_dim, depths[i], num_heads[i], window_size)\n",
    "            for i in range(len(depths))\n",
    "        ])\n",
    "        self.upsample = Upsampler(scale, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.shallow(x)\n",
    "        xb = self.body(x0)\n",
    "        return self.upsample(x0 + xb)\n",
    "\n",
    "# ---------- Losses ----------\n",
    "class CharbonnierLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, sr, hr):\n",
    "        diff = sr - hr\n",
    "        return torch.mean(torch.sqrt(diff*diff + self.eps**2))\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, layer_ids=[3,8,15], weights=[1.0,1.0,1.0]):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features.eval()\n",
    "        for p in vgg.parameters(): p.requires_grad=False\n",
    "        self.slices = nn.ModuleList()\n",
    "        prev = 0\n",
    "        for lid in layer_ids:\n",
    "            self.slices.append(nn.Sequential(*vgg[prev:lid]))\n",
    "            prev = lid\n",
    "        self.weights = weights\n",
    "    def forward(self, sr, hr):\n",
    "        loss = 0\n",
    "        for w, slice in zip(self.weights, self.slices):\n",
    "            loss += w * F.l1_loss(slice(sr), slice(hr))\n",
    "        return loss\n",
    "\n",
    "# ---------- Training ----------\n",
    "def train(model, loader, optimizer, device,\n",
    "          loss_type='l1', gan=None, perceptual=None,\n",
    "          epochs=25, save_path='best_swinir.pth'):\n",
    "    best_psnr = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    criterion = {\n",
    "        'l1': nn.L1Loss(),\n",
    "        'mse': nn.MSELoss(),\n",
    "        'charb': CharbonnierLoss()\n",
    "    }.get(loss_type, nn.L1Loss())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for lr, hr in loop:\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                sr = model(lr)\n",
    "                loss = criterion(sr, hr)\n",
    "                if gan and perceptual:\n",
    "                    d_real = gan.discriminator(hr)\n",
    "                    d_fake = gan.discriminator(sr.detach())\n",
    "                    adv_loss = gan.adversarial_loss(d_fake, True)\n",
    "                    p_loss = perceptual(sr, hr)\n",
    "                    loss = loss + gan.lambda_gan*adv_loss + gan.lambda_p*p_loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # validation PSNR\n",
    "        model.eval()\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr, hr = lr.to(device), hr.to(device)\n",
    "                sr = model(lr)\n",
    "                mse = F.mse_loss(sr, hr)\n",
    "                psnr = 20*torch.log10(1.0/torch.sqrt(mse))\n",
    "                psnr_sum += psnr.item(); cnt+=1\n",
    "                if cnt >= 10: break\n",
    "        avg_psnr = psnr_sum / cnt\n",
    "        print(f'Epoch {epoch+1} | PSNR: {avg_psnr:.2f}')\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = DIV2KDataset('/kaggle/input/paintings/resized_dataset/resized_dataset', '/kaggle/input/paintings/1024data')\n",
    "    loader  = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    \n",
    "    model = SwinIR(embed_dim=96,\n",
    "                   depths=[6,6,6,6],\n",
    "                   num_heads=[6,6,6,6],\n",
    "                   window_size=8, scale=4).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "    # Classical SR with L1 pixel loss\n",
    "    train(model, loader, optimizer, device, loss_type='l1', epochs=10)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T12:48:33.367037Z",
     "iopub.status.busy": "2025-04-26T12:48:33.366787Z",
     "iopub.status.idle": "2025-04-26T13:12:01.709337Z",
     "shell.execute_reply": "2025-04-26T13:12:01.707760Z",
     "shell.execute_reply.started": "2025-04-26T12:48:33.367016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Dataset (fixed resizing) ----------\n",
    "class DIV2KDataset(Dataset):\n",
    "    SUPPORTED_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "    def __init__(self, lr_dir, hr_dir, lr_size=256, hr_size=1024):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_size = lr_size\n",
    "        self.hr_size = hr_size\n",
    "        lr_files = {f for f in os.listdir(lr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        hr_files = {f for f in os.listdir(hr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        self.files = sorted(lr_files & hr_files)\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No matching images in {lr_dir} and {hr_dir}\")\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        lr = Image.open(os.path.join(self.lr_dir, fname)).convert(\"RGB\")\n",
    "        hr = Image.open(os.path.join(self.hr_dir, fname)).convert(\"RGB\")\n",
    "        lr = lr.resize((self.lr_size, self.lr_size), Image.BICUBIC)\n",
    "        hr = hr.resize((self.hr_size, self.hr_size), Image.BICUBIC)\n",
    "        return self.to_tensor(lr), self.to_tensor(hr)\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H//window_size, window_size, W//window_size, window_size)\n",
    "    windows = x.permute(0,2,4,3,5,1).contiguous().view(-1, window_size*window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H*W/window_size/window_size))\n",
    "    x = windows.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0,5,1,3,2,4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "# ---------- Model Components ----------\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans=3, embed_dim=64, patch_size=1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x): return self.proj(x)\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        self.heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.heads, C//self.heads).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1,2).reshape(B_, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=8, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        shortcut = x\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm1(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "        x = x + shortcut\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        return x\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    def __init__(self, dim, depth, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        layers = [SwinTransformerBlock(dim, num_heads, window_size) for _ in range(depth)]\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        def run(x_in):\n",
    "            out = self.blocks(x_in)\n",
    "            out = self.conv(out)\n",
    "            return out + x_in\n",
    "        return checkpoint.checkpoint(run, x)\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, scale, n_feats):\n",
    "        m = []\n",
    "        if (scale & (scale-1)) == 0:\n",
    "            for _ in range(int(math.log2(scale))):\n",
    "                m += [nn.Conv2d(n_feats, n_feats*4, 3,1,1), nn.PixelShuffle(2)]\n",
    "        elif scale == 3:\n",
    "            m += [nn.Conv2d(n_feats, n_feats*9, 3,1,1), nn.PixelShuffle(3)]\n",
    "        else:\n",
    "            raise ValueError(f\"Scale {scale} not supported\")\n",
    "        m += [nn.Conv2d(n_feats, 3, 3,1,1)]\n",
    "        super().__init__(*m)\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    def __init__(self, embed_dim=64, depths=[4,4], num_heads=[4,4], window_size=8, scale=4):\n",
    "        super().__init__()\n",
    "        self.shallow = PatchEmbed(3, embed_dim, 1)\n",
    "        self.body = nn.Sequential(*[\n",
    "            RSTB(embed_dim, depths[i], num_heads[i], window_size)\n",
    "            for i in range(len(depths))\n",
    "        ])\n",
    "        self.upsample = Upsampler(scale, embed_dim)\n",
    "    def forward(self, x):\n",
    "        x0 = self.shallow(x)\n",
    "        xb = self.body(x0)\n",
    "        return self.upsample(x0 + xb)\n",
    "\n",
    "# ---------- PSNR Calculation ----------\n",
    "def calc_psnr(sr, hr):\n",
    "    mse = F.mse_loss(sr, hr)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse)) if mse > 0 else torch.tensor(100.)\n",
    "\n",
    "# ---------- Training Loop (with resume) ----------\n",
    "def train(model, loader, optimizer, device, epochs=25, save_path='best_swinir.pth'):\n",
    "    best_psnr = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Resume from checkpoint if available\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading checkpoint '{save_path}'...\")\n",
    "        state_dict = torch.load(save_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        # compute its PSNR\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr, hr = lr.to(device), hr.to(device)\n",
    "                sr = model(lr)\n",
    "                psnr_sum += calc_psnr(sr, hr).item()\n",
    "                cnt += 1\n",
    "                if cnt >= 10: break\n",
    "        best_psnr = psnr_sum / cnt\n",
    "        print(f\"Resumed best PSNR: {best_psnr:.2f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for lr, hr in loop:\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                sr = model(lr)\n",
    "                loss = F.l1_loss(sr, hr)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Evaluate PSNR\n",
    "        model.eval()\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr, hr = lr.to(device), hr.to(device)\n",
    "                sr = model(lr)\n",
    "                psnr_sum += calc_psnr(sr, hr).item()\n",
    "                cnt += 1\n",
    "                if cnt >= 10: break\n",
    "        avg_psnr = psnr_sum / cnt\n",
    "        print(f\"Epoch {epoch+1} | PSNR: {avg_psnr:.2f}\")\n",
    "\n",
    "        # Save if improved\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved new best PSNR: {best_psnr:.2f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = DIV2KDataset(\n",
    "        '/kaggle/input/paintings/resized_dataset/resized_dataset',\n",
    "        '/kaggle/input/paintings/1024data'\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    model = SwinIR(\n",
    "        embed_dim=64, depths=[4,4], num_heads=[4,4], window_size=8, scale=4\n",
    "    ).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "    train(model, loader, optimizer, device, epochs=20)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T14:43:00.403Z",
     "iopub.execute_input": "2025-04-26T14:29:17.594039Z",
     "iopub.status.busy": "2025-04-26T14:29:17.593780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Dataset (random aligned crop for patch training) ----------\n",
    "class DIV2KDataset(Dataset):\n",
    "    SUPPORTED_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "    def __init__(self, lr_dir, hr_dir, hr_patch=256, scale=2):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.scale = scale\n",
    "        self.hr_patch = hr_patch\n",
    "        self.lr_patch = hr_patch // scale\n",
    "\n",
    "        # only keep matched files\n",
    "        lr_files = {f for f in os.listdir(lr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        hr_files = {f for f in os.listdir(hr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        self.files = sorted(lr_files & hr_files)\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No matching images in {lr_dir} and {hr_dir}\")\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        hr = Image.open(os.path.join(self.hr_dir, fname)).convert('RGB')\n",
    "        lr = Image.open(os.path.join(self.lr_dir, fname)).convert('RGB')\n",
    "\n",
    "        # random crop on HR\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(hr, (self.hr_patch, self.hr_patch))\n",
    "        hr_crop = TF.crop(hr, i, j, h, w)\n",
    "\n",
    "        # aligned crop on LR\n",
    "        lr_i, lr_j = i // self.scale, j // self.scale\n",
    "        lr_crop = TF.crop(lr, lr_i, lr_j, self.lr_patch, self.lr_patch)\n",
    "\n",
    "        return self.to_tensor(lr_crop), self.to_tensor(hr_crop)\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H//window_size, window_size, W//window_size, window_size)\n",
    "    windows = x.permute(0,2,4,3,5,1).contiguous().view(-1, window_size*window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H*W/window_size/window_size))\n",
    "    x = windows.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0,5,1,3,2,4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "# ---------- Model Components ----------\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans=3, embed_dim=32, patch_size=1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x): return self.proj(x)\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        self.heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.heads, C//self.heads).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1,2).reshape(B_, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=8, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)), nn.GELU(), nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "        self.window_size = window_size\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        shortcut = x\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm1(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "        x = x + shortcut\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        return x\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    def __init__(self, dim, depth, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        layers = [SwinTransformerBlock(dim, num_heads, window_size) for _ in range(depth)]\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = self.conv(out)\n",
    "        return out + x\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, scale, n_feats):\n",
    "        m = []\n",
    "        if scale == 2:\n",
    "            m += [nn.Conv2d(n_feats, n_feats*4, 3,1,1), nn.PixelShuffle(2)]\n",
    "        else:\n",
    "            raise ValueError(f\"Scale {scale} not supported\")\n",
    "        m += [nn.Conv2d(n_feats, 3, 3,1,1)]\n",
    "        super().__init__(*m)\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    def __init__(self, embed_dim=32, depths=[2,2], num_heads=[2,2], window_size=8, scale=2):\n",
    "        super().__init__()\n",
    "        self.shallow = PatchEmbed(3, embed_dim, 1)\n",
    "        self.body = nn.Sequential(*[\n",
    "            RSTB(embed_dim, depths[i], num_heads[i], window_size)\n",
    "            for i in range(len(depths))\n",
    "        ])\n",
    "        self.upsample = Upsampler(scale, embed_dim)\n",
    "    def forward(self, x):\n",
    "        x0 = self.shallow(x)\n",
    "        xb = self.body(x0)\n",
    "        return self.upsample(x0 + xb)\n",
    "\n",
    "# ---------- PSNR Calculation ----------\n",
    "def calc_psnr(sr, hr):\n",
    "    mse = F.mse_loss(sr, hr)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse)) if mse > 0 else torch.tensor(100.)\n",
    "\n",
    "# ---------- Training Loop (with resume) ----------\n",
    "def train(model, loader, optimizer, device, epochs=25, save_path='best_swinir_x2.pth'):\n",
    "    best_psnr = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        state = torch.load(save_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.eval()\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr = lr.to(device, non_blocking=True)\n",
    "                hr = hr.to(device, non_blocking=True)\n",
    "                sr = model(lr)\n",
    "                psnr_sum += calc_psnr(sr, hr).item()\n",
    "                cnt += 1\n",
    "                if cnt >= 10: break\n",
    "        best_psnr = psnr_sum / cnt\n",
    "        print(f\"Resumed best PSNR: {best_psnr:.2f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for lr, hr in loop:\n",
    "            lr = lr.to(device, non_blocking=True)\n",
    "            hr = hr.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                sr = model(lr)\n",
    "                loss = F.l1_loss(sr, hr)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr = lr.to(device, non_blocking=True)\n",
    "                hr = hr.to(device, non_blocking=True)\n",
    "                sr = model(lr)\n",
    "                psnr_sum += calc_psnr(sr, hr).item()\n",
    "                cnt += 1\n",
    "                if cnt >= 10: break\n",
    "        avg_psnr = psnr_sum / cnt\n",
    "        print(f\"Epoch {epoch+1} | PSNR: {avg_psnr:.2f}\")\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved new best PSNR: {best_psnr:.2f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = DIV2KDataset(\n",
    "        '/kaggle/input/paintings/resized_dataset/resized_dataset',\n",
    "        '/kaggle/input/paintings/512_data',\n",
    "        hr_patch=256, scale=2\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    model = SwinIR(\n",
    "        embed_dim=32,\n",
    "        depths=[2,2],\n",
    "        num_heads=[2,2],\n",
    "        window_size=8,\n",
    "        scale=2\n",
    "    ).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "    train(model, loader, optimizer, device, epochs=20)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T09:17:19.832277Z",
     "iopub.status.busy": "2025-04-27T09:17:19.832009Z",
     "iopub.status.idle": "2025-04-27T09:17:19.919362Z",
     "shell.execute_reply": "2025-04-27T09:17:19.918368Z",
     "shell.execute_reply.started": "2025-04-27T09:17:19.832256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Dataset (random aligned crop for patch training) ----------\n",
    "class DIV2KDataset(Dataset):\n",
    "    SUPPORTED_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "    def _init_(self, lr_dir, hr_dir, hr_patch=256, scale=2):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.scale = scale\n",
    "        self.hr_patch = hr_patch\n",
    "        self.lr_patch = hr_patch // scale\n",
    "\n",
    "        # only keep matched files\n",
    "        lr_files = {f for f in os.listdir(lr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        hr_files = {f for f in os.listdir(hr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        self.files = sorted(lr_files & hr_files)\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No matching images in {lr_dir} and {hr_dir}\")\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        hr = Image.open(os.path.join(self.hr_dir, fname)).convert('RGB')\n",
    "        lr = Image.open(os.path.join(self.lr_dir, fname)).convert('RGB')\n",
    "\n",
    "        # random crop on HR\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(hr, (self.hr_patch, self.hr_patch))\n",
    "        hr_crop = TF.crop(hr, i, j, h, w)\n",
    "\n",
    "        # aligned crop on LR\n",
    "        lr_i, lr_j = i // self.scale, j // self.scale\n",
    "        lr_crop = TF.crop(lr, lr_i, lr_j, self.lr_patch, self.lr_patch)\n",
    "\n",
    "        return self.to_tensor(lr_crop), self.to_tensor(hr_crop)\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H//window_size, window_size, W//window_size, window_size)\n",
    "    windows = x.permute(0,2,4,3,5,1).contiguous().view(-1, window_size*window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H*W/window_size/window_size))\n",
    "    x = windows.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0,5,1,3,2,4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "# ---------- Model Components ----------\n",
    "class PatchEmbed(nn.Module):\n",
    "    def _init_(self, in_chans=3, embed_dim=32, patch_size=1):\n",
    "        super()._init_()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x): return self.proj(x)\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def _init_(self, dim, num_heads, window_size):\n",
    "        super()._init_()\n",
    "        self.heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.heads, C//self.heads).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1,2).reshape(B_, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def _init_(self, dim, num_heads, window_size=8, mlp_ratio=4.0):\n",
    "        super()._init_()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)), nn.GELU(), nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "        self.window_size = window_size\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        shortcut = x\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm1(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "        x = x + shortcut\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        return x\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    def _init_(self, dim, depth, num_heads, window_size):\n",
    "        super()._init_()\n",
    "        layers = [SwinTransformerBlock(dim, num_heads, window_size) for _ in range(depth)]\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = self.conv(out)\n",
    "        return out + x\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def _init_(self, scale, n_feats):\n",
    "        m = []\n",
    "        if scale == 2:\n",
    "            m += [nn.Conv2d(n_feats, n_feats*4, 3,1,1), nn.PixelShuffle(2)]\n",
    "        else:\n",
    "            raise ValueError(f\"Scale {scale} not supported\")\n",
    "        m += [nn.Conv2d(n_feats, 3, 3,1,1)]\n",
    "        super()._init_(*m)\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    def _init_(self, embed_dim=32, depths=[2,2], num_heads=[2,2], window_size=8, scale=2):\n",
    "        super()._init_()\n",
    "        self.shallow = PatchEmbed(3, embed_dim, 1)\n",
    "        self.body = nn.Sequential(*[\n",
    "            RSTB(embed_dim, depths[i], num_heads[i], window_size)\n",
    "            for i in range(len(depths))\n",
    "        ])\n",
    "        self.upsample = Upsampler(scale, embed_dim)\n",
    "    def forward(self, x):\n",
    "        x0 = self.shallow(x)\n",
    "        xb = self.body(x0)\n",
    "        return self.upsample(x0 + xb)\n",
    "\n",
    "# ---------- PSNR Calculation ----------\n",
    "def calc_psnr(sr, hr):\n",
    "    mse = F.mse_loss(sr, hr)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse)) if mse > 0 else torch.tensor(100.)\n",
    "\n",
    "# ---------- Training Loop (with resume) ----------\n",
    "def train(model, loader, optimizer, device, epochs=25, save_path='best_swinir_x2.pth'):\n",
    "    best_psnr = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    if os.path.exists(RESUME_CHECKPOINT):\n",
    "        state = torch.load(RESUME_CHECKPOINT, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.eval()\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr = lr.to(device, non_blocking=True)\n",
    "                hr = hr.to(device, non_blocking=True)\n",
    "                sr = model(lr)\n",
    "                psnr_sum += calc_psnr(sr, hr).item()\n",
    "                cnt += 1\n",
    "                if cnt >= 10: break\n",
    "        best_psnr = psnr_sum / cnt\n",
    "        print(f\"Resumed best PSNR: {best_psnr:.2f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for lr, hr in loop:\n",
    "            lr = lr.to(device, non_blocking=True)\n",
    "            hr = hr.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                sr = model(lr)\n",
    "                loss = F.l1_loss(sr, hr)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr = lr.to(device, non_blocking=True)\n",
    "                hr = hr.to(device, non_blocking=True)\n",
    "                sr = model(lr)\n",
    "                psnr_sum += calc_psnr(sr, hr).item()\n",
    "                cnt += 1\n",
    "                if cnt >= 10: break\n",
    "        avg_psnr = psnr_sum / cnt\n",
    "        print(f\"Epoch {epoch+1} | PSNR: {avg_psnr:.2f}\")\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved new best PSNR: {best_psnr:.2f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = DIV2KDataset(\n",
    "        '/kaggle/input/paintings/resized_dataset/resized_dataset',\n",
    "        '/kaggle/input/paintings/512_data',\n",
    "        hr_patch=256, scale=2\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    model = SwinIR(\n",
    "        embed_dim=128,\n",
    "        depths=[6,6,6,6],\n",
    "        num_heads=[8,8,8,8],\n",
    "        window_size=16,\n",
    "        scale=2\n",
    "    ).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "    train(model, loader, optimizer, device, epochs=20)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-27T13:09:08.762Z",
     "iopub.execute_input": "2025-04-27T13:07:51.264464Z",
     "iopub.status.busy": "2025-04-27T13:07:51.264248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "RESUME_CHECKPOINT ='/kaggle/input/ipaths/best_swinir_x2.pth'\n",
    "BEST_MODEL_PATH  =' /kaggle/working/best_swinir_x2.pth'\n",
    "# ---------- Dataset (random aligned crop for patch training) ----------\n",
    "class DIV2KDataset(Dataset):\n",
    "    SUPPORTED_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "    def __init__(self, lr_dir, hr_dir, hr_patch=256, scale=2):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.scale = scale\n",
    "        self.hr_patch = hr_patch\n",
    "        self.lr_patch = hr_patch // scale\n",
    "\n",
    "        # only keep matched files\n",
    "        lr_files = {f for f in os.listdir(lr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        hr_files = {f for f in os.listdir(hr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        self.files = sorted(lr_files & hr_files)\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No matching images in {lr_dir} and {hr_dir}\")\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        hr = Image.open(os.path.join(self.hr_dir, fname)).convert('RGB')\n",
    "        lr = Image.open(os.path.join(self.lr_dir, fname)).convert('RGB')\n",
    "\n",
    "        # random crop on HR\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(hr, (self.hr_patch, self.hr_patch))\n",
    "        hr_crop = TF.crop(hr, i, j, h, w)\n",
    "\n",
    "        # aligned crop on LR\n",
    "        lr_i, lr_j = i // self.scale, j // self.scale\n",
    "        lr_crop = TF.crop(lr, lr_i, lr_j, self.lr_patch, self.lr_patch)\n",
    "\n",
    "        return self.to_tensor(lr_crop), self.to_tensor(hr_crop)\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H//window_size, window_size, W//window_size, window_size)\n",
    "    windows = x.permute(0,2,4,3,5,1).contiguous().view(-1, window_size*window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H*W/window_size/window_size))\n",
    "    x = windows.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0,5,1,3,2,4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "# ---------- Model Components ----------\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans=3, embed_dim=32, patch_size=1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x): return self.proj(x)\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        self.heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.heads, C//self.heads).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1,2).reshape(B_, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=8, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)), nn.GELU(), nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "        self.window_size = window_size\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        shortcut = x\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm1(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "        x = x + shortcut\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        return x\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    def __init__(self, dim, depth, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        layers = [SwinTransformerBlock(dim, num_heads, window_size) for _ in range(depth)]\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = self.conv(out)\n",
    "        return out + x\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, scale, n_feats):\n",
    "        m = []\n",
    "        if scale == 2:\n",
    "            m += [nn.Conv2d(n_feats, n_feats*4, 3,1,1), nn.PixelShuffle(2)]\n",
    "        else:\n",
    "            raise ValueError(f\"Scale {scale} not supported\")\n",
    "        m += [nn.Conv2d(n_feats, 3, 3,1,1)]\n",
    "        super().__init__(*m)\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    def __init__(self, embed_dim=32, depths=[2,2], num_heads=[2,2], window_size=8, scale=2):\n",
    "        super().__init__()\n",
    "        self.shallow = PatchEmbed(3, embed_dim, 1)\n",
    "        self.body = nn.Sequential(*[\n",
    "            RSTB(embed_dim, depths[i], num_heads[i], window_size)\n",
    "            for i in range(len(depths))\n",
    "        ])\n",
    "        self.upsample = Upsampler(scale, embed_dim)\n",
    "    def forward(self, x):\n",
    "        x0 = self.shallow(x)\n",
    "        xb = self.body(x0)\n",
    "        return self.upsample(x0 + xb)\n",
    "\n",
    "# ---------- PSNR Calculation ----------\n",
    "def calc_psnr(sr, hr):\n",
    "    mse = F.mse_loss(sr, hr)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse)) if mse > 0 else torch.tensor(100.)\n",
    "\n",
    "# ---------- Training Loop (with resume) ----------\n",
    "def train(model, loader, optimizer, device, epochs=25, save_path='best_swinir_x2.pth'):\n",
    "    best_psnr = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    if os.path.exists(RESUME_CHECKPOINT):\n",
    "        state = torch.load(RESUME_CHECKPOINT, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.eval()\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr = lr.to(device, non_blocking=True)\n",
    "                hr = hr.to(device, non_blocking=True)\n",
    "                sr = model(lr)\n",
    "                psnr_sum += calc_psnr(sr, hr).item()\n",
    "                cnt += 1\n",
    "                if cnt >= 10: break\n",
    "        best_psnr = psnr_sum / cnt\n",
    "        print(f\"Resumed best PSNR: {best_psnr:.2f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for lr, hr in loop:\n",
    "            lr = lr.to(device, non_blocking=True)\n",
    "            hr = hr.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                sr = model(lr)\n",
    "                loss = F.l1_loss(sr, hr)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        psnr_sum, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in loader:\n",
    "                lr = lr.to(device, non_blocking=True)\n",
    "                hr = hr.to(device, non_blocking=True)\n",
    "                sr = model(lr)\n",
    "                psnr_sum += calc_psnr(sr, hr).item()\n",
    "                cnt += 1\n",
    "                if cnt >= 10: break\n",
    "        avg_psnr = psnr_sum / cnt\n",
    "        print(f\"Epoch {epoch+1} | PSNR: {avg_psnr:.2f}\")\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved new best PSNR: {best_psnr:.2f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = DIV2KDataset(\n",
    "        '/kaggle/input/paintings/resized_dataset/resized_dataset',\n",
    "        '/kaggle/input/paintings/512_data',\n",
    "        hr_patch=256, scale=2\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    model = SwinIR(\n",
    "        embed_dim=128,\n",
    "        depths=[6,6,6,6],\n",
    "        num_heads=[8,8,8,8],\n",
    "        window_size=16,\n",
    "        scale=2\n",
    "    ).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "    train(model, loader, optimizer, device, epochs=20)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T06:27:46.027782Z",
     "iopub.status.busy": "2025-04-28T06:27:46.027503Z",
     "iopub.status.idle": "2025-04-28T08:35:36.862918Z",
     "shell.execute_reply": "2025-04-28T08:35:36.861417Z",
     "shell.execute_reply.started": "2025-04-28T06:27:46.027760Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from /kaggle/input/ipaths/best_swinir_x2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/85461441.py:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(RESUME_CHECKPOINT, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed best PSNR: 4.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1981/1981 [23:57<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | PSNR: 13.15\n",
      "Saved new best PSNR: 13.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1981/1981 [23:56<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | PSNR: 13.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1981/1981 [23:56<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | PSNR: 12.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1981/1981 [23:56<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | PSNR: 12.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 1981/1981 [23:56<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | PSNR: 12.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:  33%|███▎      | 650/1981 [07:51<16:06,  1.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/85461441.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31/85461441.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, scheduler, device, epochs)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Checkpoint paths ----------\n",
    "RESUME_CHECKPOINT = '/kaggle/input/ipaths/best_swinir_x2.pth'\n",
    "BEST_MODEL_PATH  = '/kaggle/working/best_swinir_x2.pth'\n",
    "BATCH_SIZE       = 2       # match your original batch size\n",
    "LR               = 2e-4    # learning rate for optimizer\n",
    "EPOCHS           = 20      # number of epochs\n",
    "\n",
    "# ---------- Dataset (aligned crop + augment) ----------\n",
    "class DIV2KDataset(Dataset):\n",
    "    SUPPORTED_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "    def __init__(self, lr_dir, hr_dir, hr_patch=256, scale=2):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.scale = scale\n",
    "        self.hr_patch = hr_patch\n",
    "        self.lr_patch = hr_patch // scale\n",
    "\n",
    "        lr_files = {f for f in os.listdir(lr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        hr_files = {f for f in os.listdir(hr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)}\n",
    "        self.files = sorted(lr_files & hr_files)\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No matching images in {lr_dir} and {hr_dir}\")\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomVerticalFlip(0.5),\n",
    "            transforms.RandomRotation(90)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        hr = Image.open(os.path.join(self.hr_dir, fname)).convert('RGB')\n",
    "        lr = Image.open(os.path.join(self.lr_dir, fname)).convert('RGB')\n",
    "\n",
    "        i, j, _, _ = transforms.RandomCrop.get_params(hr, (self.hr_patch, self.hr_patch))\n",
    "        hr_crop = TF.crop(hr, i, j, self.hr_patch, self.hr_patch)\n",
    "        lr_crop = TF.crop(lr, i // self.scale, j // self.scale, self.lr_patch, self.lr_patch)\n",
    "\n",
    "        # same augmentation on both\n",
    "        hr_crop = self.augment(hr_crop)\n",
    "        lr_crop = self.augment(lr_crop)\n",
    "\n",
    "        return self.to_tensor(lr_crop), self.to_tensor(hr_crop)\n",
    "\n",
    "# ---------- SwinIR Model (global residual skip) ----------\n",
    "def window_partition(x, window_size):\n",
    "    B,C,H,W = x.shape\n",
    "    x = x.view(B,C,H//window_size,window_size,W//window_size,window_size)\n",
    "    return x.permute(0,2,4,3,5,1).reshape(-1, window_size*window_size, C)\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = windows.shape[0] // (H*W//window_size//window_size)\n",
    "    x = windows.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    return x.permute(0,5,1,3,2,4).reshape(B, -1, H, W)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans=3, embed_dim=32, patch_size=1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, patch_size, patch_size)\n",
    "    def forward(self, x): return self.proj(x)\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        self.heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.heads, C//self.heads).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1,2).reshape(B_, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=8, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)), nn.GELU(), nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "        self.window_size = window_size\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        shortcut = x\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm1(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "        x = x + shortcut\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        return x\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    def __init__(self, dim, depth, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        layers = [SwinTransformerBlock(dim, num_heads, window_size) for _ in range(depth)]\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = self.conv(out)\n",
    "        return out + x\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, scale, n_feats):\n",
    "        m = []\n",
    "        if scale == 2:\n",
    "            m += [nn.Conv2d(n_feats, n_feats*4, 3,1,1), nn.PixelShuffle(2)]\n",
    "        else:\n",
    "            raise ValueError(f\"Scale {scale} not supported\")\n",
    "        m += [nn.Conv2d(n_feats, 3, 3,1,1)]\n",
    "        super().__init__(*m)\n",
    "\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    def __init__(self, embed_dim=32, depths=[2,2], num_heads=[2,2], window_size=8, scale=2):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.shallow = PatchEmbed(3, embed_dim, 1)\n",
    "        self.body = nn.Sequential(*[\n",
    "            RSTB(embed_dim, depths[i], num_heads[i], window_size)\n",
    "            for i in range(len(depths))\n",
    "        ])\n",
    "        self.upsample = Upsampler(scale, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.shallow(x)\n",
    "        xb = self.body(x0)\n",
    "        feat = x0 + xb\n",
    "        sr = self.upsample(feat)\n",
    "        base = F.interpolate(x, scale_factor=self.scale, mode='bicubic', align_corners=False)\n",
    "        return sr + base\n",
    "\n",
    "# ---------- PSNR Calc ----------\n",
    "def calc_psnr(sr, hr):\n",
    "    mse = F.mse_loss(sr, hr)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse)) if mse > 0 else torch.tensor(100.)\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "def train(model, loader, optimizer, scheduler, device, epochs=20):\n",
    "    best_psnr = 0.0\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    # resume\n",
    "    if os.path.exists(RESUME_CHECKPOINT):\n",
    "        print(f\"Loading pretrained weights from {RESUME_CHECKPOINT}\")\n",
    "        model.load_state_dict(torch.load(RESUME_CHECKPOINT, map_location=device))\n",
    "        model.eval()\n",
    "        psnr_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (lr, hr) in enumerate(loader):\n",
    "                if i >= 10: break\n",
    "                lr, hr = lr.to(device), hr.to(device)\n",
    "                psnr_sum += calc_psnr(model(lr), hr).item()\n",
    "        best_psnr = psnr_sum / 10\n",
    "        print(f\"Resumed best PSNR: {best_psnr:.2f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for lr, hr in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                sr = model(lr)\n",
    "                loss = F.mse_loss(sr, hr)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        psnr_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (lr, hr) in enumerate(loader):\n",
    "                if i >= 10: break\n",
    "                lr, hr = lr.to(device), hr.to(device)\n",
    "                psnr_sum += calc_psnr(model(lr), hr).item()\n",
    "        avg_psnr = psnr_sum / 10\n",
    "        print(f\"Epoch {epoch+1} | PSNR: {avg_psnr:.2f}\")\n",
    "\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(f\"Saved new best PSNR: {best_psnr:.2f}\")\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = DIV2KDataset(\n",
    "        '/kaggle/input/paintings/resized_dataset/resized_dataset',\n",
    "        '/kaggle/input/paintings/512_data', hr_patch=256, scale=2\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    model = SwinIR(embed_dim=128, depths=[4,4], num_heads=[8,8,8,8], window_size=16, scale=2).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    total_steps = 20 * len(loader)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LR, total_steps=total_steps, pct_start=0.05, anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    train(model, loader, optimizer, scheduler, device, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:41:37.934053Z",
     "iopub.status.busy": "2025-04-28T08:41:37.933476Z",
     "iopub.status.idle": "2025-04-28T11:59:19.673016Z",
     "shell.execute_reply": "2025-04-28T11:59:19.672207Z",
     "shell.execute_reply.started": "2025-04-28T08:41:37.934028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/119731980.py:166: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Train Epoch 1/50:   0%|          | 0/892 [00:00<?, ?it/s]/tmp/ipykernel_31/119731980.py:187: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train Epoch 1/50: 100%|██████████| 892/892 [03:51<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 1 PSNR (Y): 26.35\n",
      "Saved new best PSNR: 26.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 2 PSNR (Y): 25.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 3 PSNR (Y): 25.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 4 PSNR (Y): 25.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 5 PSNR (Y): 25.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 6 PSNR (Y): 25.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 7 PSNR (Y): 26.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 8 PSNR (Y): 25.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 9 PSNR (Y): 25.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 10 PSNR (Y): 25.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 11 PSNR (Y): 25.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 12 PSNR (Y): 25.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 13 PSNR (Y): 25.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 14 PSNR (Y): 25.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 15/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 15 PSNR (Y): 26.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 16/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 16 PSNR (Y): 25.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 17/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 17 PSNR (Y): 25.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 18/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 18 PSNR (Y): 25.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 19/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 19 PSNR (Y): 25.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 20/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 20 PSNR (Y): 26.38\n",
      "Saved new best PSNR: 26.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 21/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 21 PSNR (Y): 25.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 22/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 22 PSNR (Y): 25.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 23/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 23 PSNR (Y): 25.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 24/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 24 PSNR (Y): 26.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 25/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 25 PSNR (Y): 25.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 26/50: 100%|██████████| 892/892 [03:50<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 26 PSNR (Y): 26.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 27/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 27 PSNR (Y): 25.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 28/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 28 PSNR (Y): 26.72\n",
      "Saved new best PSNR: 26.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 29/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 29 PSNR (Y): 26.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 30/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 30 PSNR (Y): 26.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 31/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 31 PSNR (Y): 26.85\n",
      "Saved new best PSNR: 26.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 32/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 32 PSNR (Y): 26.94\n",
      "Saved new best PSNR: 26.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 33/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 33 PSNR (Y): 26.98\n",
      "Saved new best PSNR: 26.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 34/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 34 PSNR (Y): 26.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 35/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 35 PSNR (Y): 26.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 36/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 36 PSNR (Y): 26.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 37/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 37 PSNR (Y): 27.19\n",
      "Saved new best PSNR: 27.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 38/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 38 PSNR (Y): 26.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 39/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 39 PSNR (Y): 27.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 40/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 40 PSNR (Y): 27.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 41/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 41 PSNR (Y): 27.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 42/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 42 PSNR (Y): 27.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 43/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 43 PSNR (Y): 27.36\n",
      "Saved new best PSNR: 27.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 44/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 44 PSNR (Y): 26.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 45/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 45 PSNR (Y): 27.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 46/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 46 PSNR (Y): 27.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 47/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 47 PSNR (Y): 27.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 48/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 48 PSNR (Y): 27.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 49/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 49 PSNR (Y): 27.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 50/50: 100%|██████████| 892/892 [03:50<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 50 PSNR (Y): 27.01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Checkpoint & Hyperparameters ----------\n",
    "#RESUME_CHECKPOINT = '/kaggle/input/ipaths/best_swinir_x2.pth'\n",
    "RESUME_CHECKPOINT = False\n",
    "BEST_MODEL_PATH  = '/kaggle/working/best_swinir_x2.pth'\n",
    "BATCH_SIZE       = 4\n",
    "LR               = 2e-4\n",
    "EPOCHS           = 50\n",
    "VAL_SPLIT        = 0.1  # fraction for validation\n",
    "\n",
    "# ---------- Dataset (aligned random crop, no augment) ----------\n",
    "class DIV2KDataset(Dataset):\n",
    "    SUPPORTED_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "    def __init__(self, lr_dir, hr_dir, hr_patch=256, scale=2):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.scale = scale\n",
    "        self.hr_patch = hr_patch\n",
    "        self.lr_patch = hr_patch // scale\n",
    "\n",
    "        lr_files = [f for f in os.listdir(lr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)]\n",
    "        hr_files = [f for f in os.listdir(hr_dir) if f.lower().endswith(self.SUPPORTED_EXTS)]\n",
    "        self.files = sorted(set(lr_files) & set(hr_files))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No matching images in {lr_dir} and {hr_dir}\")\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        hr = Image.open(os.path.join(self.hr_dir, fname)).convert('RGB')\n",
    "        lr = Image.open(os.path.join(self.lr_dir, fname)).convert('RGB')\n",
    "\n",
    "        i, j, _, _ = transforms.RandomCrop.get_params(hr, (self.hr_patch, self.hr_patch))\n",
    "        hr_crop = TF.crop(hr, i, j, self.hr_patch, self.hr_patch)\n",
    "        lr_crop = TF.crop(lr, i // self.scale, j // self.scale, self.lr_patch, self.lr_patch)\n",
    "\n",
    "        return self.to_tensor(lr_crop), self.to_tensor(hr_crop)\n",
    "\n",
    "# ---------- SwinIR Model (global residual skip) ----------\n",
    "def window_partition(x, window_size):\n",
    "    B,C,H,W = x.shape\n",
    "    x = x.view(B,C,H//window_size,window_size,W//window_size,window_size)\n",
    "    return x.permute(0,2,4,3,5,1).reshape(-1, window_size*window_size, C)\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = windows.shape[0] // (H*W//window_size//window_size)\n",
    "    x = windows.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    return x.permute(0,5,1,3,2,4).reshape(B, -1, H, W)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans=3, embed_dim=32, patch_size=1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, patch_size, patch_size)\n",
    "    def forward(self, x): return self.proj(x)\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        self.heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.heads, C//self.heads).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1,2).reshape(B_, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=8, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)), nn.GELU(), nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "        self.window_size = window_size\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        shortcut = x\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm1(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "        x = x + shortcut\n",
    "        x = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x).reshape(B, H, W, C).permute(0,3,1,2)\n",
    "        return x\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    def __init__(self, dim, depth, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        layers = [SwinTransformerBlock(dim, num_heads, window_size) for _ in range(depth)]\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = self.conv(out)\n",
    "        return out + x\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, scale, n_feats):\n",
    "        m = []\n",
    "        if scale == 2:\n",
    "            m += [nn.Conv2d(n_feats, n_feats*4, 3,1,1), nn.PixelShuffle(2)]\n",
    "        else:\n",
    "            raise ValueError(f\"Scale {scale} not supported\")\n",
    "        m += [nn.Conv2d(n_feats, 3, 3,1,1)]\n",
    "        super().__init__(*m)\n",
    "\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    def __init__(self, embed_dim=32, depths=[2,2], num_heads=[2,2], window_size=8, scale=2):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.shallow = PatchEmbed(3, embed_dim, 1)\n",
    "        self.body = nn.Sequential(*[\n",
    "            RSTB(embed_dim, depths[i], num_heads[i], window_size)\n",
    "            for i in range(len(depths))\n",
    "        ])\n",
    "        self.upsample = Upsampler(scale, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.shallow(x)\n",
    "        xb = self.body(x0)\n",
    "        feat = x0 + xb\n",
    "        sr = self.upsample(feat)\n",
    "        base = F.interpolate(x, scale_factor=self.scale, mode='bicubic', align_corners=False)\n",
    "        return sr + base\n",
    "\n",
    "# ---------- Luma PSNR Calculation ----------\n",
    "def calc_psnr_y(sr, hr):\n",
    "    # sr, hr: [B,3,H,W] in [0,1]\n",
    "    # convert to luma channel\n",
    "    coeff = torch.tensor([0.299, 0.587, 0.114], device=sr.device).view(1,3,1,1)\n",
    "    sr_y = (sr * coeff).sum(dim=1, keepdim=True)\n",
    "    hr_y = (hr * coeff).sum(dim=1, keepdim=True)\n",
    "    mse = F.mse_loss(sr_y, hr_y)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse)) if mse > 0 else torch.tensor(100.)\n",
    "\n",
    "# ---------- Training Loop with Validation ----------\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, device, epochs=EPOCHS):\n",
    "    best_psnr = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # resume\n",
    "    if RESUME_CHECKPOINT and os.path.exists(RESUME_CHECKPOINT):\n",
    "        print(f\"Loading pretrained weights from {RESUME_CHECKPOINT}\")\n",
    "        model.load_state_dict(torch.load(RESUME_CHECKPOINT, map_location=device))\n",
    "        model.eval()\n",
    "        psnr_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in val_loader:\n",
    "                lr, hr = lr.to(device), hr.to(device)\n",
    "                psnr_sum += calc_psnr_y(model(lr), hr).item()\n",
    "        best_psnr = psnr_sum / len(val_loader)\n",
    "        print(f\"Resumed best PSNR on Y: {best_psnr:.2f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for lr, hr in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{epochs}\"):\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                sr = model(lr)\n",
    "                loss = F.mse_loss(sr, hr)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        psnr_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in val_loader:\n",
    "                lr, hr = lr.to(device), hr.to(device)\n",
    "                psnr_sum += calc_psnr_y(model(lr), hr).item()\n",
    "        avg_psnr = psnr_sum / len(val_loader)\n",
    "        print(f\"Val Epoch {epoch+1} PSNR (Y): {avg_psnr:.2f}\")\n",
    "\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(f\"Saved new best PSNR: {best_psnr:.2f}\")\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    dataset = DIV2KDataset(\n",
    "        '/kaggle/input/paintings/resized_dataset/resized_dataset',\n",
    "        '/kaggle/input/paintings/512_data',\n",
    "        hr_patch=256, scale=2\n",
    "    )\n",
    "    # train/val split\n",
    "    val_size = int(len(dataset) * VAL_SPLIT)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = SwinIR(embed_dim=96, depths=[4,4], num_heads=[4,4], window_size=4, scale=2).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    total_steps = EPOCHS * len(train_loader)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=LR,\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.05,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, device)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7219522,
     "sourceId": 11512903,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7218899,
     "sourceId": 11576795,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7264270,
     "sourceId": 11585599,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
