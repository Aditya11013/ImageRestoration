{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11567214,"sourceType":"datasetVersion","datasetId":7218899}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data preperation for Swinir","metadata":{}},{"cell_type":"markdown","source":"code written by kushwanth","metadata":{}},{"cell_type":"code","source":"!pip install basicsr  huggingface_hub transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:09:02.470378Z","iopub.execute_input":"2025-04-22T16:09:02.470577Z","iopub.status.idle":"2025-04-22T16:11:11.513011Z","shell.execute_reply.started":"2025-04-22T16:09:02.470553Z","shell.execute_reply":"2025-04-22T16:11:11.512299Z"}},"outputs":[{"name":"stdout","text":"Collecting basicsr\n  Downloading basicsr-1.4.2.tar.gz (172 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting addict (from basicsr)\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from basicsr) (1.0.0)\nCollecting lmdb (from basicsr)\n  Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from basicsr) (1.26.4)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from basicsr) (4.11.0.86)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from basicsr) (11.1.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from basicsr) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from basicsr) (2.32.3)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from basicsr) (0.25.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from basicsr) (1.15.2)\nCollecting tb-nightly (from basicsr)\n  Downloading tb_nightly-2.20.0a20250422-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.11/dist-packages (from basicsr) (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from basicsr) (0.20.1+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from basicsr) (4.67.1)\nCollecting yapf (from basicsr)\n  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->basicsr) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->basicsr) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->basicsr) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->basicsr) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->basicsr) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->basicsr) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7->basicsr)\n  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7->basicsr)\n  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7->basicsr)\n  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7->basicsr)\n  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7->basicsr)\n  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7->basicsr)\n  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7->basicsr)\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->basicsr) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7->basicsr) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->basicsr) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->basicsr) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->basicsr) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->basicsr) (2025.1.31)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->basicsr) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->basicsr) (2025.1.10)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->basicsr) (0.4)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->basicsr) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->basicsr) (1.70.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->basicsr) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->basicsr) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->basicsr) (75.1.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->basicsr) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->basicsr) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->basicsr) (3.1.3)\nRequirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->basicsr) (4.3.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tb-nightly->basicsr) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->basicsr) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->basicsr) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->basicsr) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->basicsr) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->basicsr) (2024.2.0)\nUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\nUsing cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\nUsing cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\nUsing cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\nUsing cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\nUsing cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\nUsing cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\nDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nDownloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tb_nightly-2.20.0a20250422-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: basicsr\n  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for basicsr: filename=basicsr-1.4.2-py3-none-any.whl size=214819 sha256=5b68f1159804f468b9e3d3143eef04f7c123a71392e58efe29d787a7b0be0c3d\n  Stored in directory: /root/.cache/pip/wheels/6d/a4/b3/9f888ba88efcae6dd4bbce69832363de9c4051142674f779fa\nSuccessfully built basicsr\nInstalling collected packages: lmdb, addict, yapf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tb-nightly, basicsr\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed addict-2.4.0 basicsr-1.4.2 lmdb-1.6.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tb-nightly-2.20.0a20250422 yapf-0.43.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# tried to down sample and then build hr sr pair","metadata":{}},{"cell_type":"code","source":"# -----------------------------------------------\n# Swin2SR Super-Resolution: 64→256 Training, 256→1024 Inference\n# -----------------------------------------------\nimport os, glob\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\nfrom transformers import (\n    AutoImageProcessor,\n    Swin2SRForImageSuperResolution\n)\nfrom torch.amp import autocast, GradScaler\n\n# -----------------------------------------------\n# Paths & Hyperparameters\n# -----------------------------------------------\nHR_FULL_DIR    = '/kaggle/input/paintings/resized_dataset'\nCHECKPOINT_DIR = '/kaggle/working/checkpoints'\nOUTPUT_DIR     = '/kaggle/working/outputs'\n\nMODEL_NAME        = 'caidas/swin2SR-classical-sr-x4-64'\nEPOCHS            = 100\nBATCH_SIZE        = 8\nLR                = 1e-4\nNUM_WORKERS       = 2\nTRAIN_CROP_SIZE   = 64\nINFERENCE_SIZE    = 256\nSCALE             = 2\n\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# -----------------------------------------------\n# Processor & Model\n# -----------------------------------------------\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME, do_pad=False)\nmodel     = Swin2SRForImageSuperResolution.from_pretrained(MODEL_NAME).to(device)\nmodel.train()\n\n# -----------------------------------------------\n# Dataset & Loader\n# -----------------------------------------------\nclass PatchDataset(Dataset):\n    def __init__(self, folder, crop_size=64, scale=4):\n        self.image_paths = glob.glob(os.path.join(folder, '*.png')) + glob.glob(os.path.join(folder, '*.jpg'))\n        self.crop_size = crop_size\n        self.scale = scale\n        self.hr_transform = transforms.Compose([\n            transforms.RandomCrop(crop_size),\n            transforms.ToTensor()\n        ])\n\n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert('RGB')\n        hr  = self.hr_transform(img)\n        lr  = transforms.Resize(self.crop_size // self.scale)(hr)\n        return {'lr': lr, 'hr': hr}\n\n    def __len__(self):\n        return len(self.image_paths)\n\ntrain_ds = PatchDataset(HR_FULL_DIR, crop_size=TRAIN_CROP_SIZE, scale=SCALE)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n\n# -----------------------------------------------\n# Training Setup\n# -----------------------------------------------\ncriterion = nn.L1Loss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nscaler    = GradScaler(device='cuda')\n\n# -----------------------------------------------\n# Training Loop\n# -----------------------------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    epoch_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n\n    for batch in pbar:\n        lr = batch['lr'].to(device)\n        hr = batch['hr'].to(device)\n\n        pixel_values = processor(images=lr, return_tensors=\"pt\").pixel_values.to(device)\n\n        optimizer.zero_grad()\n        with autocast(device_type='cuda'):\n            out = model(pixel_values=pixel_values)\n            sr  = out.reconstruction\n            loss = criterion(sr, hr)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        epoch_loss += loss.item()\n        pbar.set_postfix(loss=loss.item())\n\n    torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/swin2sr_epoch{epoch+1}.pt\")\n\n# -----------------------------------------------\n# Inference on Full Images\n# -----------------------------------------------\nmodel.eval()\nimage_paths = glob.glob(os.path.join(HR_FULL_DIR, '*.png')) + glob.glob(os.path.join(HR_FULL_DIR, '*.jpg'))\n\nwith torch.no_grad():\n    for path in tqdm(image_paths, desc='Inference'):\n        img = Image.open(path).convert(\"RGB\").resize((INFERENCE_SIZE, INFERENCE_SIZE))\n        inputs = processor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n\n        with autocast(device_type='cuda'):\n            out = model(pixel_values=inputs)\n            sr_img = out.reconstruction.clamp(0, 1)\n\n        fname = os.path.basename(path)\n        save_image(sr_img, os.path.join(OUTPUT_DIR, fname))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T11:21:34.723497Z","iopub.execute_input":"2025-04-22T11:21:34.724267Z","iopub.status.idle":"2025-04-22T14:32:29.596361Z","shell.execute_reply.started":"2025-04-22T11:21:34.724242Z","shell.execute_reply":"2025-04-22T14:32:29.594490Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/100:   0%|          | 0/496 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\nEpoch 1/100: 100%|██████████| 496/496 [01:54<00:00,  4.33it/s, loss=0.15] \nEpoch 2/100: 100%|██████████| 496/496 [01:54<00:00,  4.34it/s, loss=0.177] \nEpoch 3/100: 100%|██████████| 496/496 [01:54<00:00,  4.35it/s, loss=0.169] \nEpoch 4/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.145] \nEpoch 5/100: 100%|██████████| 496/496 [01:51<00:00,  4.43it/s, loss=0.153] \nEpoch 6/100: 100%|██████████| 496/496 [01:54<00:00,  4.34it/s, loss=0.127] \nEpoch 7/100: 100%|██████████| 496/496 [01:51<00:00,  4.45it/s, loss=0.0902]\nEpoch 8/100: 100%|██████████| 496/496 [01:54<00:00,  4.34it/s, loss=0.0523]\nEpoch 9/100: 100%|██████████| 496/496 [01:52<00:00,  4.40it/s, loss=0.0838]\nEpoch 10/100: 100%|██████████| 496/496 [01:52<00:00,  4.43it/s, loss=0.124] \nEpoch 11/100: 100%|██████████| 496/496 [01:53<00:00,  4.37it/s, loss=0.109] \nEpoch 12/100: 100%|██████████| 496/496 [01:51<00:00,  4.44it/s, loss=0.24]  \nEpoch 13/100: 100%|██████████| 496/496 [01:51<00:00,  4.46it/s, loss=0.114] \nEpoch 14/100: 100%|██████████| 496/496 [01:51<00:00,  4.45it/s, loss=0.108] \nEpoch 15/100: 100%|██████████| 496/496 [01:53<00:00,  4.37it/s, loss=0.168] \nEpoch 16/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.0338]\nEpoch 17/100: 100%|██████████| 496/496 [01:51<00:00,  4.46it/s, loss=0.0484]\nEpoch 18/100: 100%|██████████| 496/496 [01:51<00:00,  4.44it/s, loss=0.0505]\nEpoch 19/100: 100%|██████████| 496/496 [01:51<00:00,  4.47it/s, loss=0.221] \nEpoch 20/100: 100%|██████████| 496/496 [01:51<00:00,  4.45it/s, loss=0.0855]\nEpoch 21/100: 100%|██████████| 496/496 [01:50<00:00,  4.47it/s, loss=0.0613]\nEpoch 22/100: 100%|██████████| 496/496 [01:50<00:00,  4.48it/s, loss=0.0751]\nEpoch 23/100: 100%|██████████| 496/496 [01:51<00:00,  4.44it/s, loss=0.0912]\nEpoch 24/100: 100%|██████████| 496/496 [01:50<00:00,  4.49it/s, loss=0.155] \nEpoch 25/100: 100%|██████████| 496/496 [01:51<00:00,  4.45it/s, loss=0.189] \nEpoch 26/100: 100%|██████████| 496/496 [01:49<00:00,  4.55it/s, loss=0.0974]\nEpoch 27/100: 100%|██████████| 496/496 [01:47<00:00,  4.60it/s, loss=0.117] \nEpoch 28/100: 100%|██████████| 496/496 [01:47<00:00,  4.60it/s, loss=0.0346]\nEpoch 29/100: 100%|██████████| 496/496 [01:50<00:00,  4.50it/s, loss=0.0797]\nEpoch 30/100: 100%|██████████| 496/496 [01:49<00:00,  4.54it/s, loss=0.0639]\nEpoch 31/100: 100%|██████████| 496/496 [01:47<00:00,  4.60it/s, loss=0.0959]\nEpoch 32/100: 100%|██████████| 496/496 [01:50<00:00,  4.49it/s, loss=0.114] \nEpoch 33/100: 100%|██████████| 496/496 [01:47<00:00,  4.60it/s, loss=0.189] \nEpoch 34/100: 100%|██████████| 496/496 [01:46<00:00,  4.64it/s, loss=0.143] \nEpoch 35/100: 100%|██████████| 496/496 [01:50<00:00,  4.51it/s, loss=0.0828]\nEpoch 36/100: 100%|██████████| 496/496 [01:51<00:00,  4.45it/s, loss=0.0962]\nEpoch 37/100: 100%|██████████| 496/496 [01:48<00:00,  4.59it/s, loss=0.0652]\nEpoch 38/100: 100%|██████████| 496/496 [01:51<00:00,  4.46it/s, loss=0.0664]\nEpoch 39/100: 100%|██████████| 496/496 [01:47<00:00,  4.60it/s, loss=0.0719]\nEpoch 40/100: 100%|██████████| 496/496 [01:48<00:00,  4.57it/s, loss=0.104] \nEpoch 41/100: 100%|██████████| 496/496 [01:48<00:00,  4.57it/s, loss=0.0641]\nEpoch 42/100: 100%|██████████| 496/496 [01:50<00:00,  4.50it/s, loss=0.0915]\nEpoch 43/100: 100%|██████████| 496/496 [01:49<00:00,  4.52it/s, loss=0.107] \nEpoch 44/100: 100%|██████████| 496/496 [01:50<00:00,  4.49it/s, loss=0.102] \nEpoch 45/100: 100%|██████████| 496/496 [01:48<00:00,  4.57it/s, loss=0.115] \nEpoch 46/100: 100%|██████████| 496/496 [01:48<00:00,  4.59it/s, loss=0.0892]\nEpoch 47/100: 100%|██████████| 496/496 [01:47<00:00,  4.61it/s, loss=0.0806]\nEpoch 48/100: 100%|██████████| 496/496 [01:52<00:00,  4.39it/s, loss=0.114] \nEpoch 49/100: 100%|██████████| 496/496 [01:54<00:00,  4.35it/s, loss=0.0407]\nEpoch 50/100: 100%|██████████| 496/496 [01:53<00:00,  4.39it/s, loss=0.0777]\nEpoch 51/100: 100%|██████████| 496/496 [01:53<00:00,  4.38it/s, loss=0.0855]\nEpoch 52/100: 100%|██████████| 496/496 [01:52<00:00,  4.41it/s, loss=0.0738]\nEpoch 53/100: 100%|██████████| 496/496 [01:55<00:00,  4.29it/s, loss=0.0655]\nEpoch 54/100: 100%|██████████| 496/496 [01:54<00:00,  4.34it/s, loss=0.105] \nEpoch 55/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.0453]\nEpoch 56/100: 100%|██████████| 496/496 [01:54<00:00,  4.32it/s, loss=0.0891]\nEpoch 57/100: 100%|██████████| 496/496 [01:54<00:00,  4.33it/s, loss=0.0778]\nEpoch 58/100: 100%|██████████| 496/496 [01:53<00:00,  4.35it/s, loss=0.0675]\nEpoch 59/100: 100%|██████████| 496/496 [01:56<00:00,  4.26it/s, loss=0.0441]\nEpoch 60/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.0954]\nEpoch 61/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.178] \nEpoch 62/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.0852]\nEpoch 63/100: 100%|██████████| 496/496 [01:55<00:00,  4.30it/s, loss=0.149] \nEpoch 64/100: 100%|██████████| 496/496 [01:55<00:00,  4.31it/s, loss=0.139] \nEpoch 65/100: 100%|██████████| 496/496 [01:55<00:00,  4.31it/s, loss=0.0471]\nEpoch 66/100: 100%|██████████| 496/496 [01:53<00:00,  4.35it/s, loss=0.104] \nEpoch 67/100: 100%|██████████| 496/496 [01:54<00:00,  4.33it/s, loss=0.174] \nEpoch 68/100: 100%|██████████| 496/496 [01:54<00:00,  4.34it/s, loss=0.0611]\nEpoch 69/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.0427]\nEpoch 70/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.0683]\nEpoch 71/100: 100%|██████████| 496/496 [01:53<00:00,  4.35it/s, loss=0.0911]\nEpoch 72/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.129] \nEpoch 73/100: 100%|██████████| 496/496 [01:53<00:00,  4.37it/s, loss=0.258] \nEpoch 74/100: 100%|██████████| 496/496 [01:53<00:00,  4.37it/s, loss=0.0354]\nEpoch 75/100: 100%|██████████| 496/496 [01:53<00:00,  4.35it/s, loss=0.0605]\nEpoch 76/100: 100%|██████████| 496/496 [01:56<00:00,  4.27it/s, loss=0.0704]\nEpoch 77/100: 100%|██████████| 496/496 [01:53<00:00,  4.37it/s, loss=0.0622]\nEpoch 78/100: 100%|██████████| 496/496 [01:54<00:00,  4.33it/s, loss=0.0501]\nEpoch 79/100: 100%|██████████| 496/496 [01:53<00:00,  4.36it/s, loss=0.133] \nEpoch 80/100: 100%|██████████| 496/496 [01:54<00:00,  4.34it/s, loss=0.0274]\nEpoch 81/100: 100%|██████████| 496/496 [01:53<00:00,  4.37it/s, loss=0.124] \nEpoch 82/100: 100%|██████████| 496/496 [01:53<00:00,  4.37it/s, loss=0.0896]\nEpoch 83/100: 100%|██████████| 496/496 [01:48<00:00,  4.55it/s, loss=0.0765]\nEpoch 84/100: 100%|██████████| 496/496 [01:50<00:00,  4.49it/s, loss=0.0897]\nEpoch 85/100: 100%|██████████| 496/496 [01:49<00:00,  4.52it/s, loss=0.096] \nEpoch 86/100: 100%|██████████| 496/496 [01:50<00:00,  4.50it/s, loss=0.0561]\nEpoch 87/100: 100%|██████████| 496/496 [01:51<00:00,  4.44it/s, loss=0.12]  \nEpoch 88/100: 100%|██████████| 496/496 [01:50<00:00,  4.48it/s, loss=0.0691]\nEpoch 89/100: 100%|██████████| 496/496 [01:50<00:00,  4.50it/s, loss=0.0809]\nEpoch 90/100: 100%|██████████| 496/496 [01:50<00:00,  4.50it/s, loss=0.0783]\nEpoch 91/100: 100%|██████████| 496/496 [01:50<00:00,  4.49it/s, loss=0.111] \nEpoch 92/100: 100%|██████████| 496/496 [01:50<00:00,  4.49it/s, loss=0.0463]\nEpoch 93/100: 100%|██████████| 496/496 [01:50<00:00,  4.47it/s, loss=0.0725]\nEpoch 94/100: 100%|██████████| 496/496 [01:50<00:00,  4.51it/s, loss=0.04]  \nEpoch 95/100: 100%|██████████| 496/496 [01:49<00:00,  4.53it/s, loss=0.103] \nEpoch 96/100: 100%|██████████| 496/496 [01:57<00:00,  4.24it/s, loss=0.113] \nEpoch 97/100: 100%|██████████| 496/496 [01:56<00:00,  4.27it/s, loss=0.0971]\nEpoch 98/100: 100%|██████████| 496/496 [01:54<00:00,  4.34it/s, loss=0.0659]\nEpoch 99/100: 100%|██████████| 496/496 [01:49<00:00,  4.51it/s, loss=0.0361]\nEpoch 100/100: 100%|██████████| 496/496 [01:50<00:00,  4.49it/s, loss=0.0798]\nInference:   6%|▌         | 245/3961 [03:56<59:47,  1.04it/s]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/926372906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0msr_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin2sr/modeling_swin2sr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1148\u001b[0m             )\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         outputs = self.swin2sr(\n\u001b[0m\u001b[1;32m   1151\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin2sr/modeling_swin2sr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    898\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin2sr/modeling_swin2sr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    725\u001b[0m                 )\n\u001b[1;32m    726\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin2sr/modeling_swin2sr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin2sr/modeling_swin2sr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin2sr/modeling_swin2sr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"# Used *4 swin ir to build hr lr data set","metadata":{}},{"cell_type":"code","source":"# In a Kaggle notebook, install dependencies in a separate cell:\n# ```bash\n# !pip install transformers pillow tqdm torchvision\n# ```\n\nimport os\nimport glob\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import Swin2SRImageProcessor, Swin2SRForImageSuperResolution\nfrom torchvision.transforms import ToPILImage\n\n# -----------------------------------------------\n# Directories & Device\n# -----------------------------------------------\nINPUT_DIR  = '/kaggle/input/paintings/resized_dataset'\nOUTPUT_DIR = '/kaggle/working/outputs'\nMODEL_NAME = 'caidas/swin2sr-classical-sr-x4-64'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# -----------------------------------------------\n# Load Processor & Model\n# -----------------------------------------------\n# Swin2SRImageProcessor handles normalization & padding\nprocessor = Swin2SRImageProcessor(\n    do_rescale=True,\n    rescale_factor=1/255.0,\n    do_pad=True,\n    pad_size=8\n)\n# Load pretrained Swin2SR model\nmodel = Swin2SRForImageSuperResolution.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Helper to convert tensor to PIL\nto_pil = ToPILImage()\n\n# -----------------------------------------------\n# Inference Loop\n# -----------------------------------------------\nimage_paths = glob.glob(os.path.join(INPUT_DIR, '*.png')) + glob.glob(os.path.join(INPUT_DIR, '*.jpg'))\n\nwith torch.no_grad():\n    for img_path in tqdm(image_paths, desc='Super-resolving'):\n        # Load low-res image\n        img = Image.open(img_path).convert('RGB')\n\n        # Preprocess: returns {'pixel_values': tensor}\n        inputs = processor(images=img, return_tensors='pt')\n        # Move tensors to device\n        pixel_values = inputs['pixel_values'].to(device)\n\n        # Forward pass\n        outputs = model(pixel_values=pixel_values)\n        # `outputs.reconstruction` is a tensor [1, C, H, W]\n        recon = outputs.reconstruction.squeeze(0).cpu().clamp(0, 1)\n\n        # Convert to PIL image\n        sr_img = to_pil(recon)\n\n        # Save SR image\n        fname = os.path.basename(img_path)\n        sr_img.save(os.path.join(OUTPUT_DIR, fname))\n\nprint(f\"Super-resolution completed. Outputs saved to: {OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T13:15:14.983830Z","iopub.execute_input":"2025-04-26T13:15:14.984527Z","iopub.status.idle":"2025-04-26T13:15:16.430468Z","shell.execute_reply.started":"2025-04-26T13:15:14.984503Z","shell.execute_reply":"2025-04-26T13:15:16.429741Z"}},"outputs":[{"name":"stderr","text":"Super-resolving: 0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Super-resolution completed. Outputs saved to: /kaggle/working/outputs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Used *2 swin ir to build hr lr pair","metadata":{}},{"cell_type":"code","source":"# In a Kaggle notebook, first install dependencies:\n# ```bash\n# !pip install transformers pillow tqdm torchvision\n# ```\n\nimport os\nimport glob\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import Swin2SRImageProcessor, Swin2SRForImageSuperResolution\nfrom torchvision.transforms import ToPILImage\n\n# -----------------------------------------------\n# Directories & Device\n# -----------------------------------------------\nINPUT_DIR  = '/kaggle/input/paintings/resized_dataset/resized_dataset'\nOUTPUT_DIR = '/kaggle/working/outputs_x2'\nMODEL_NAME = 'caidas/swin2sr-classical-sr-x2-64'   # ← x2 model instead of x4\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# -----------------------------------------------\n# Load Processor & Model\n# -----------------------------------------------\nprocessor = Swin2SRImageProcessor(\n    do_rescale=True,\n    rescale_factor=1/255.0,\n    do_pad=True,\n    pad_size=8\n)\nmodel = Swin2SRForImageSuperResolution.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\nto_pil = ToPILImage()\n\n# -----------------------------------------------\n# Inference Loop\n# -----------------------------------------------\nimage_paths = (\n    glob.glob(os.path.join(INPUT_DIR, '*.png')) +\n    glob.glob(os.path.join(INPUT_DIR, '*.jpg'))\n)\n\nwith torch.no_grad():\n    for img_path in tqdm(image_paths, desc='Super-resolving ×2'):\n        # Load\n        img = Image.open(img_path).convert('RGB')\n\n        # Preprocess\n        inputs = processor(images=img, return_tensors='pt')\n        pixel_values = inputs['pixel_values'].to(device)\n\n        # Forward\n        outputs = model(pixel_values=pixel_values)\n        recon = outputs.reconstruction.squeeze(0).cpu().clamp(0, 1)\n\n        # Save\n        sr_img = to_pil(recon)\n        fname  = os.path.basename(img_path)\n        sr_img.save(os.path.join(OUTPUT_DIR, fname))\n\nprint(f\"×2 Super-resolution done. Outputs in {OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T13:18:20.824225Z","iopub.execute_input":"2025-04-26T13:18:20.824933Z","iopub.status.idle":"2025-04-26T14:15:49.609401Z","shell.execute_reply.started":"2025-04-26T13:18:20.824907Z","shell.execute_reply":"2025-04-26T14:15:49.608667Z"}},"outputs":[{"name":"stderr","text":"Super-resolving ×2: 100%|██████████| 3961/3961 [57:27<00:00,  1.15it/s]","output_type":"stream"},{"name":"stdout","text":"×2 Super-resolution done. Outputs in /kaggle/working/outputs_x2\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import shutil\nzip_base = '/kaggle/working/outputs_x2'\nshutil.make_archive(zip_base, 'zip', OUTPUT_DIR)\nzip_path = zip_base + '.zip'\n\nprint(f\"Super-resolution completed. Outputs saved to: {OUTPUT_DIR}\")\nprint(f\"Zipped outputs to: {zip_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T14:15:56.384027Z","iopub.execute_input":"2025-04-26T14:15:56.384475Z","iopub.status.idle":"2025-04-26T14:16:02.407238Z","shell.execute_reply.started":"2025-04-26T14:15:56.384450Z","shell.execute_reply":"2025-04-26T14:16:02.406613Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Super-resolution completed. Outputs saved to: /kaggle/working/outputs_x2\nZipped outputs to: /kaggle/working/outputs_x2.zip\n","output_type":"stream"}],"execution_count":6}]}
